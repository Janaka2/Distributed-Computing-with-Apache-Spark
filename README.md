# Distributed Computing using Apache Spark

## Overview
This repository demonstrates how to use Apache Spark for distributed data processing. It includes examples of data manipulation, machine learning, and graph processing using the Spark framework.

## Features
- Data manipulation using Spark DataFrames and SQL
- Machine learning with Spark MLlib
- Graph processing with Spark GraphX
- Cluster deployment using Docker and Kubernetes

## Technologies
- Python
- Apache Spark
- Docker
- Kubernetes

## Setup and Run
1. Clone this repository
2. Install dependencies: `pip install -r requirements.txt`
3. Run the example scripts: `spark-submit [example_script].py`

## Fun Fact
This project began as an exploration of distributed computing and quickly evolved into an appreciation for the power and versatility of Apache Spark. I enjoyed learning how to harness the power of multiple machines to process large amounts of data.

# I'm Janaka Premathilaka,
ðŸ“« Feel free to reach out to me at janaka2@gmail.com , on [LinkedIn](https://www.linkedin.com/in/janakap/) or give me a call at +41 76 224 84 45. ðŸ’Œ ðŸš€
